{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9h9A9g4WeY_"
   },
   "source": [
    "# Лабораторна робота №5. Семантична сегментація"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvnyGb2oW2NY"
   },
   "source": [
    "**Виконала** студентка групи КІ-51мп Додонова Марія\n",
    "\n",
    "**Мета:** Навчитись вирiшувати задачу семантичної сегментацiї зображень, дослiдити та реалiзувати архiтектури типу Encoder-Decoder, зокрема U-Net, а також ознайомитись iз сучасними foundation-моделями для сегментацiї."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8_VMk6DXLlT"
   },
   "source": [
    "## Порядок виконання"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmWcUkVeXPVy"
   },
   "source": [
    "1. Обрати фреймворк для виконання роботи: `tensorflow` чи `pytorch`.\n",
    "\n",
    "2. Обрати датасет для вирiшення задачi семантичної сегментацiї. Рекомендується почати з одного з наступних:\n",
    "* [Oxford-IIIT Pet Dataset:](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet) Стандартний датасет для сегментацiї, що мiстить зображення домашнiх тварин 37 порiд.\n",
    "* [Cityscapes Dataset:](https://www.cityscapes-dataset.com/) Зображення мiських пейзажiв. Бiльш складний та об’ємний.\n",
    "* [МРТ-скани головного мозку:](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset) Приклад медичного датасету.\n",
    "\n",
    "3. Реалiзувати модель U-Net власноруч, використовуючи стандартнi шари вашого фреймворку. Звернiть особливу увагу на реалiзацiю skip connections.\n",
    "\n",
    "4. Навчити обрану модель на датасетi. Спробуйте використати специфiчну для сегментацiї функцiю втрат (наприклад, Dice Loss) та порiвняйте її результати зi стандартною Cross-Entropy.\n",
    "\n",
    "5. Проаналiзувати графiки навчання. Оцiнити якiсть моделi за допомогою метрики mIoU на валiдацiйнiй вибiрцi. Обрати одну з архiтектур, реалiзованих у лабораторнiй роботi №2 або №3 (наприклад, вашу власну просту CNN, VGG або ResNet), яка буде слугувати базовою моделлю для експериментiв.\n",
    "\n",
    "6. Вiзуалiзувати результати роботи моделi: для кiлькох зображень з тестової вибiрки показати оригiнальне зображення, справжню маску сегментацiї та маску, згенеровану вашою моделлю.\n",
    "\n",
    "* **Завдання на додатковi бали:** Дослiдити одну з сучасних foundation моделей для сегментацiї. Оберiть один з варiантiв:\n",
    "\n",
    "  * **Segment Anything Model (SAM) / SAM 2:** Використовуючи [офiцiйну демо-версiю](https://segment-anything.com/) або [готовий код](https://github.com/facebookresearch/segment-anything), застосуйте модель до кiлькох власних зображень. Опишiть, як працює prompt-based segmentation (сегментацiя за пiдказками: точки, рамки).\n",
    "  * **Zero-shot Segmentation:** Дослiдiть моделi, що можуть сегментувати об’єкти за текстовим описом без попереднього навчання на конкретних класах (наприклад, на основi CLIP). Реалiзуйте приклад zero-shot сегментацiї, використовуючи одну з [доступних моделей на Hugging Face](https://huggingface.co/models?pipeline_tag=zero-shot-image-segmentation&sort=trending)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH5eCl4VXwrL"
   },
   "source": [
    "## Виконання роботи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Обрати фреймворк для виконання роботи: `tensorflow` чи `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10027,
     "status": "ok",
     "timestamp": 1733169014181,
     "user": {
      "displayName": "Mariia Dodonova",
      "userId": "17823790577590824865"
     },
     "user_tz": -120
    },
    "id": "mn9TJaGIX97z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from src.train.trainer import Trainer\n",
    "from src.losses.diceloss import DiceLoss\n",
    "from src.plot_tools import plot_history, imshow\n",
    "from src.models.unet import UNet\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Обрати датасет для вирiшення задачi семантичної сегментацiї. Рекомендується почати з одного з наступних:\n",
    "* [Oxford-IIIT Pet Dataset:](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet) Стандартний датасет для сегментацiї, що мiстить зображення домашнiх тварин 37 порiд.\n",
    "* [Cityscapes Dataset:](https://www.cityscapes-dataset.com/) Зображення мiських пейзажiв. Бiльш складний та об’ємний.\n",
    "* [МРТ-скани головного мозку:](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset) Приклад медичного датасету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetToLong(nn.Module):\n",
    "    def forward(self, target: torch.Tensor) -> torch.Tensor:\n",
    "        target = (target * 255 - 1).to(torch.long)\n",
    "        return target\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    TargetToLong()\n",
    "])\n",
    "\n",
    "# Load train split\n",
    "train_dataset = datasets.OxfordIIITPet(\n",
    "    root=DATA_PATH,\n",
    "    split=\"trainval\",\n",
    "    target_types=\"segmentation\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "# Load test split\n",
    "test_dataset = datasets.OxfordIIITPet(\n",
    "    root=DATA_PATH,\n",
    "    split=\"test\",\n",
    "    target_types=\"segmentation\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "# Split train into train and validation\n",
    "train_dataset, val_dataset = random_split(train_dataset, [0.8, 0.2])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYD33NLfX_9d"
   },
   "source": [
    "### 3. Реалiзувати модель U-Net власноруч, використовуючи стандартнi шари вашого фреймворку. Звернiть особливу увагу на реалiзацiю skip connections.\n",
    "\n",
    "### 4. Навчити обрану модель на датасетi. Спробуйте використати специфiчну для сегментацiї функцiю втрат (наприклад, Dice Loss) та порiвняйте її результати зi стандартною Cross-Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training U-Net with DiceLoss:\")\n",
    "dl_criterion = DiceLoss()\n",
    "dl_unet = UNet(in_channels=3)\n",
    "dl_optimizer = optim.Adam(dl_unet.parameters(), lr=LEARNING_RATE)\n",
    "dl_trainer = Trainer(dl_unet, dl_optimizer, dl_criterion, device=DEVICE)\n",
    "dl_history = dl_trainer.train(NUM_EPOCHS, train_loader, val_loader)\n",
    "print(\"Test results:\", end=\" \")\n",
    "dl_trainer.test(test_loader)\n",
    "\n",
    "print(\"\\nTraining U-Net with CrossEntropyLoss:\")\n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "ce_unet = UNet(in_channels=3)\n",
    "ce_optimizer = optim.Adam(ce_unet.parameters(), lr=LEARNING_RATE)\n",
    "ce_trainer = Trainer(ce_unet, ce_optimizer, ce_criterion, device=DEVICE)\n",
    "ce_history = ce_trainer.train(NUM_EPOCHS, train_loader, val_loader)\n",
    "print(\"Test results:\", end=\" \")\n",
    "ce_trainer.test(test_loader)\n",
    "\n",
    "print(\"\\nFinished Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-d6cFInYJNg"
   },
   "source": [
    "### 5. Проаналiзувати графiки навчання. Оцiнити якiсть моделi за допомогою метрики mIoU на валiдацiйнiй вибiрцi. Обрати одну з архiтектур, реалiзованих у лабораторнiй роботi №2 або №3 (наприклад, вашу власну просту CNN, VGG або ResNet), яка буде слугувати базовою моделлю для експериментiв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"U-Net (DiceLoss) Curves\")\n",
    "plot_history(dl_history)\n",
    "\n",
    "print(\"U-Net (DiceLoss) Curves\")\n",
    "plot_history(ce_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Вiзуалiзувати результати роботи моделi: для кiлькох зображень з тестової вибiрки показати оригiнальне зображення, справжню маску сегментацiї та маску, згенеровану вашою моделлю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_loader:\n",
    "    dl_unet.eval()\n",
    "    ce_unet.eval()\n",
    "    images = images.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        dl_out = dl_unet(images)\n",
    "        ce_out = ce_unet(images)\n",
    "    \n",
    "    images = images.to(\"cpu\").permute(0, 2, 3, 1).numpy()\n",
    "    labels = labels.permute(0, 2, 3, 1).numpy()\n",
    "    dl_pred = torch.argmax(dl_out, dim=1).to(\"cpu\").unsqueeze(-1).numpy()\n",
    "    ce_pred = torch.argmax(ce_out, dim=1).to(\"cpu\").unsqueeze(-1).numpy()\n",
    "    for i in range(3):\n",
    "        imshow(images[i], \"Input image\")\n",
    "        imshow(labels[i], \"Label\")\n",
    "        imshow(dl_pred[i], \"Prediction (DiceLoss)\")\n",
    "        imshow(ce_pred[i], \"Prediction (CrossEntropyLoss)\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * **Завдання на додатковi бали:** Дослiдити одну з сучасних foundation моделей для сегментацiї. Оберiть один з варiантiв:\n",
    "\n",
    "  * **Segment Anything Model (SAM) / SAM 2:** Використовуючи [офiцiйну демо-версiю](https://segment-anything.com/) або [готовий код](https://github.com/facebookresearch/segment-anything), застосуйте модель до кiлькох власних зображень. Опишiть, як працює prompt-based segmentation (сегментацiя за пiдказками: точки, рамки)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Zero-shot Segmentation:** Дослiдiть моделi, що можуть сегментувати об’єкти за текстовим описом без попереднього навчання на конкретних класах (наприклад, на основi CLIP). Реалiзуйте приклад zero-shot сегментацiї, використовуючи одну з [доступних моделей на Hugging Face](https://huggingface.co/models?pipeline_tag=zero-shot-image-segmentation&sort=trending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEXG0pErYZyD"
   },
   "source": [
    "## Висновок"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOXPdtH2gmLy7VEhHupdpmR",
   "collapsed_sections": [
    "z8_VMk6DXLlT"
   ],
   "mount_file_id": "1p86IQ13AuUlL447SF636iKsY5nbrQaEw",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "iasa-computer-vision-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
